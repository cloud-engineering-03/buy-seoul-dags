import requests
import datetime
import pandas as pd
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
from sqlalchemy.engine import make_url
import psycopg2
from airflow.models import Variable


db_url = "postgresql+psycopg2://test:test@postgres-postgresql.postgres.svc.cluster.local:5432/testdb"


def fetch_raw_data(**context):
    api_key = Variable.get("SEOUL_API_KEY")
    service_name = "tbLnOpendataRtmsV"
    base_url = "http://openapi.seoul.go.kr:8088"

    step = 100  # API ìµœëŒ€ ë°˜í™˜ ê±´ìˆ˜ ê¶Œì¥ ë‹¨ìœ„
    today = datetime.today()
    year = int(today.strftime("%Y"))
    start_date = datetime(year, 1, 1)
    end_date = datetime(year, 12, 31)

    all_rows = []

    # for month in range(1, 13):
    #     deal_ymd = f"{year}{str(month).zfill(2)}"
    #     for start in range(1, 10000, step):  # ìµœëŒ€ 10,000ê±´ê¹Œì§€ í˜ì´ì§•
    #         end = start + step - 1
    #         url = f"{base_url}/{api_key}/json/{service_name}/{start}/{end}/"
    #         params = {"DEAL_YMD": deal_ymd}
    #         response = requests.get(url, params=params)
    #         if response.status_code != 200:
    #             print(f"[{deal_ymd}] ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
    #             break
    #         data = response.json()
    #         items = data.get(service_name, {}).get("row", [])
    #         if not items:
    #             break  # ë” ì´ìƒ ì—†ìŒ
    #         all_rows.extend(items)

    # if not all_rows:
    #     print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    #     return
    while start_date <= end_date:
        date_str = start_date.strftime("%Y%m%d")  # CTRT_DAY í˜•ì‹: YYYYMMDD
        print(f"ğŸ“… ìš”ì²­ ë‚ ì§œ: {date_str}")

        for start in range(1, 10, step):
            end = start + step - 1
            url = f"{base_url}/{api_key}/json/{service_name}/{start}/{end}/%20/%20/%20/%20/%20/%20/%20/%20/%20/{date_str}/%20/"
            params = {"CTRT_DAY": date_str}

            response = requests.get(url, params=params)
            if response.status_code != 200:
                print(f"âŒ [{date_str}] ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                break

            data = response.json()
            items = data.get(service_name, {}).get("row", [])

            if not items:
                break  # ì´ ë‚ ì§œì— ë” ì´ìƒ ì—†ìŒ

            all_rows.extend(items)

        start_date += timedelta(days=1)  # ë‹¤ìŒ ë‚ ë¡œ ë„˜ì–´ê°

    df = pd.DataFrame(all_rows)

    column_mapping = {
        "RTRCN_DAY": "ì·¨ì†Œì¼", "LAND_AREA": "í† ì§€ë©´ì ", "STDG_CD": "ë²•ì •ë™ì½”ë“œ",
        "BLDG_NM": "ê±´ë¬¼ëª…", "STDG_NM": "ë²•ì •ë™ëª…", "MNO": "ë³¸ë²ˆ",
        "THING_AMT": "ë¬¼ê±´ê¸ˆì•¡", "LOTNO_SE_NM": "ì§€ë²ˆêµ¬ë¶„ëª…", "LOTNO_SE": "ì§€ë²ˆêµ¬ë¶„",
        "CTRT_DAY": "ê³„ì•½ì¼", "RCPT_YR": "ì ‘ìˆ˜ì—°ë„", "OPBIZ_RESTAGNT_SGG_NM": "ì‹ ê³ í•œ ê°œì—…ê³µì¸ì¤‘ê°œì‚¬ ì‹œêµ°êµ¬ëª…",
        "ARCH_AREA": "ê±´ë¬¼ë©´ì ", "CGG_CD": "ìì¹˜êµ¬ì½”ë“œ", "RGHT_SE": "ê¶Œë¦¬êµ¬ë¶„",
        "SNO": "ë¶€ë²ˆ", "FLR": "ì¸µ", "CGG_NM": "ìì¹˜êµ¬ëª…",
        "BLDG_USG": "ê±´ë¬¼ìš©ë„", "ARCH_YR": "ê±´ì¶•ë…„ë„", "DCLR_SE": "ì‹ ê³ êµ¬ë¶„"
    }
    df.rename(columns=column_mapping, inplace=True)

    # XComìœ¼ë¡œ ì „ë‹¬
    context['ti'].xcom_push(key='raw_df', value=df.to_json())
    print(f"[âœ… ìˆ˜ì§‘ ì™„ë£Œ] ì´ ìˆ˜ì§‘ í–‰ ìˆ˜: {len(df)}")


def validate_data():
    # df ë¶ˆëŸ¬ì™€ ìœ íš¨ì„± ê²€ì‚¬
    ...


def insert_data(**context):
    ti = context["ti"]
    raw_json = ti.xcom_pull(task_ids='fetch_raw_data', key='raw_df')
    df = pd.read_json(raw_json)

    # ì „ì²˜ë¦¬
    # ë¹ˆ ë¬¸ìì—´, ê³µë°±, 'null' ë¬¸ìì—´ì„ Noneìœ¼ë¡œ ë³€í™˜
    df = df.drop(columns=["ìì¹˜êµ¬ëª…"], errors="ignore")
    df = df.drop(columns=["ë²•ì •ë™ëª…"], errors="ignore")
    for col in ["ê³„ì•½ì¼", "ì·¨ì†Œì¼"]:
        df[col] = df[col].replace(['', ' ', 'null', 'N/A'], None)
        df[col] = df[col].apply(lambda x: pd.to_datetime(
            x, format='%Y%m%d', errors='coerce') if x is not None else None)
        df[col] = df[col].apply(lambda x: None if pd.isna(x) else x)

    # ì „ì—­ None/ê²°ì¸¡ ì²˜ë¦¬
    df = df.replace([pd.NaT, '', ' ', 'null', 'N/A'], None)

    df = df.rename(columns={
        "ìì¹˜êµ¬ì½”ë“œ": "district_code", "ë²•ì •ë™ì½”ë“œ": "legal_dong_code",
        "ê³„ì•½ì¼": "contract_date", "ì·¨ì†Œì¼": "cancellation_date",
        "ì§€ë²ˆêµ¬ë¶„": "lot_type", "ì§€ë²ˆêµ¬ë¶„ëª…": "lot_type_name",
        "ë³¸ë²ˆ": "main_lot_number", "ë¶€ë²ˆ": "sub_lot_number",
        "ê±´ë¬¼ëª…": "building_name", "ì¸µ": "floor",
        "ê±´ë¬¼ë©´ì ": "building_area", "í† ì§€ë©´ì ": "land_area",
        "ë¬¼ê±´ê¸ˆì•¡": "transaction_amount", "ê±´ë¬¼ìš©ë„": "building_usage",
        "ê±´ì¶•ë…„ë„": "construction_year", "ì‹ ê³ êµ¬ë¶„": "report_type",
        "ê¶Œë¦¬êµ¬ë¶„": "ownership_type", "ì ‘ìˆ˜ì—°ë„": "report_year",
        "ì‹ ê³ í•œ ê°œì—…ê³µì¸ì¤‘ê°œì‚¬ ì‹œêµ°êµ¬ëª…": "agent_office_district_name"
    })
    df = df[[  # Reorder and subset to ensure correct columns
        "district_code", "legal_dong_code", "contract_date", "cancellation_date", "lot_type", "lot_type_name",
        "main_lot_number", "sub_lot_number", "building_name", "floor", "building_area", "land_area",
        "transaction_amount", "building_usage", "construction_year", "report_type", "ownership_type",
        "report_year", "agent_office_district_name"
    ]]

    url = make_url(db_url)
    conn = psycopg2.connect(
        dbname=url.database,
        user=url.username,
        password=url.password,
        host=url.host,
        port=url.port
    )

    bulk_insert_data(df, "public.real_estate_transaction",
                     conn, chunk_size=100)


def prune_old_data():
    engine = create_engine(db_url)

    cutoff = (datetime.today() - timedelta(days=3)).strftime("%Y%m%d")

    with engine.connect() as conn:
        conn.execute(text("""
            DELETE FROM public.ESTATE_DATA
            WHERE "ê³„ì•½ì¼" < :cutoff
        """), {"cutoff": cutoff})


def t5_check_table():
    engine = create_engine(db_url)

    with engine.connect() as conn:
        df = pd.read_sql("SELECT * FROM public.ESTATE_DATA", conn)
        print(df.to_string(index=False))


def bulk_insert_data(df, table_name, conn, chunk_size=100):
    cur = conn.cursor()

    columns = [
        "district_code", "legal_dong_code", "contract_date", "cancellation_date", "lot_type", "lot_type_name",
        "main_lot_number", "sub_lot_number", "building_name", "floor", "building_area", "land_area",
        "transaction_amount", "building_usage", "construction_year", "report_type", "ownership_type",
        "report_year", "agent_office_district_name"
    ]
    quoted_columns = [f'"{col}"' for col in columns]

    insert_prefix = f"INSERT INTO {table_name} ({', '.join(quoted_columns)}) VALUES "

    success_count = 0
    fail_count = 0
    buffer = []

    for idx, row in df.iterrows():
        values = []
        for col in columns:
            val = row.get(col, None)
            if pd.isna(val):
                values.append('NULL')
            elif isinstance(val, str):
                val = val.replace("'", "''")
                values.append(f"'{val}'")
            elif isinstance(val, pd.Timestamp):
                values.append(f"'{val.strftime('%Y-%m-%d %H:%M:%S')}'")
            elif val is None:
                values.append('NULL')
            else:
                values.append(str(val))

        buffer.append(f"({', '.join(values)})")

        # chunk_sizeë§ˆë‹¤ insert
        if (idx + 1) % chunk_size == 0:
            try:
                sql = insert_prefix + ",\n".join(buffer) + ";"
                cur.execute(sql)
                conn.commit()
                print(f"[âœ… {idx+1}ê°œ] ì‚½ì… ì„±ê³µ")
                success_count += len(buffer)
            except Exception as e:
                conn.rollback()
                print(f"[âŒ {idx+1}ê°œ] ì‚½ì… ì‹¤íŒ¨: {e}")
                fail_count += len(buffer)
            buffer = []
        # print(sql)
    # ë‚¨ì€ row ì²˜ë¦¬
    if buffer:
        try:
            sql = insert_prefix + ",\n".join(buffer) + ";"
            cur.execute(sql)
            conn.commit()
            print(f"[âœ… ë‚¨ì€ {len(buffer)}ê°œ] ì‚½ì… ì„±ê³µ")
            success_count += len(buffer)
        except Exception as e:
            conn.rollback()
            print(f"[âŒ ë‚¨ì€ {len(buffer)}ê°œ] ì‚½ì… ì‹¤íŒ¨: {e}")
            fail_count += len(buffer)

    cur.close()
    conn.close()

    print(f"ì´ ì„±ê³µ: {success_count} / ì‹¤íŒ¨: {fail_count}")
